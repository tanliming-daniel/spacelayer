## Overview

This project, `spacelayer`, is a pipeline for 3D reconstruction and novel view synthesis from video. It combines traditional computer vision techniques with deep learning to create a dynamic 3D representation of a scene. The core idea is to use 2D and 3D tracking data to train a neural radiance field (NeRF)-like model that can render the scene from any viewpoint.

## Core Architecture

The main pipeline is orchestrated in `demo_pipeline.py` and involves several key components:

1.  **Data Loading and Pre-processing (`demo_pipeline.py`):**
    -   Loads 2D/3D tracks, visibility predictions, and camera intrinsics from `.pt` files generated by scripts like `background_points.py`.
    -   Loads video frames and foreground masks to isolate the background for rendering.
    -   Transforms 2D track coordinates into a global coordinate system and normalizes 3D tracks to a `[-1, 1]` bounding box. This is crucial for the renderer.

2.  **Meshing and Sampling (`models/pipeline.py`, `models/tools.py`):**
    -   `TriangleMeshPipeline` (`models/pipeline.py`): Takes 2D vertices and a mask for a given frame and generates a 2D triangular mesh. This is a key step to prepare data for the neural renderer.
    -   `sample_points` (`models/tools.py`): Samples points from the generated meshes across all frames, which are then used to train the rendering model.

3.  **Neural Renderer (`models/pipeline.py`):**
    -   `WorldSpaceTrianglePipeline`: This is the core neural network, similar to a NeRF. It's an MLP that takes sampled 3D points and their associated features and learns to predict their RGB color.

4.  **Training and Inference (`demo_pipeline.py`):**
    -   **Training:** The `main` function in `demo_pipeline.py` contains the training loop. It samples points, passes them through the `WorldSpaceTrianglePipeline` renderer, computes a loss against the ground truth pixel colors, and updates the model.
    -   **Reconstruction:** After training, the model reconstructs the original video by rendering each frame.
    -   **Novel View Synthesis:** The code includes a (currently commented out) section for generating new camera trajectories (e.g., a spiral path) and rendering the scene from these new viewpoints.

## Developer Workflows

### Data Preparation

Before running the main pipeline, you need to generate the tracking data. This is done using scripts like `background_points.py` and `foreground_points.py`.

-   **Example:** `python background_points.py --video_path /path/to/video`
-   This will process the video and save the tracking data as a `.pt` file in the `results` directory.

### Running the Main Pipeline

The primary entry point for training and rendering is `demo_pipeline.py`.

-   **Command:** `python demo_pipeline.py`
-   **Configuration:** Key parameters such as learning rate, number of epochs, and data paths are configured at the top of the `main` function in `demo_pipeline.py`.
-   **Output:** The script saves trained models, loss curves, and rendered videos to the `results/pipeline_demo` directory.

## Key Conventions

-   **Coordinate Systems:** Pay close attention to the coordinate system transformations in `demo_pipeline.py`. 2D coordinates are shifted to a global system, and 3D coordinates are normalized. This is a common pattern in 3D deep learning and is essential for the model to converge.
-   **Data Flow:** The pipeline follows a clear data flow: Video -> 2D/3D Tracks -> `.pt` files -> Training -> Rendered Video. When debugging, trace the data through these stages.
-   **Hybrid Approach:** The project uniquely blends classic computer vision (Delaunay triangulation for meshing) with deep learning (neural rendering). The `TriangleMeshPipeline` is the bridge between these two worlds.
-   **Dependencies:** The project uses standard libraries like PyTorch, NumPy, and Matplotlib. Key external dependencies are listed in `requirements.txt`. The `submodules` directory may contain other important dependencies.
